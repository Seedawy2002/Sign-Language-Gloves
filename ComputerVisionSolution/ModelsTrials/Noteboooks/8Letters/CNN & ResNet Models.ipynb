{"cells":[{"cell_type":"markdown","metadata":{"id":"-dK3GrM-uCz7"},"source":["## **Introduction**\n","\n","In this notebook, Arabic sign language gestures will be recognized from images using a CNN model. Arabic Sign Language (ArSL) is a visual-gestural language used by deaf and hard-of-hearing individuals in Arab countries for communication.\n","\n","This project focuses on developing a CNN model to accurately recognize Arabic sign language gestures from images. By leveraging deep learning techniques, the model will be trained to identify various signs with high precision.\n","\n","\n","### **Notebook Overview:**\n","\n","* Data Preparation: The dataset comprising hand gesture images representing different signs in Arabic Sign Language is prepared. Images are loaded and preprocessed to ensure suitability for training the CNN model.\n","* Model Building: The CNN architecture is designed and implemented using TensorFlow and Keras. The model is trained on preprocessed images to learn patterns and features associated with different sign gestures.\n","* Model Training: The CNN model is trained on the prepared dataset, with performance monitored over multiple epochs. Techniques such as data augmentation and regularization are utilized to improve generalization.\n","* Model Evaluation & Prediction: The trained model's performance is evaluated on a separate test dataset. Metrics including accuracy, precision, recall, and F1-score are computed to assess the model's effectiveness."]},{"cell_type":"markdown","metadata":{"id":"ttOCD-vSuzJ4"},"source":["## Libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Ilw4FSk8uyOv","executionInfo":{"status":"ok","timestamp":1716222477922,"user_tz":-180,"elapsed":707,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["import os\n","import shutil\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9RZILVhrvM7x","executionInfo":{"status":"ok","timestamp":1716222481923,"user_tz":-180,"elapsed":4008,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","\n","import random\n","from random import randint\n","\n","from sklearn.utils import shuffle # Shuffle arrays or sparse matrices in a consistent way\n","from sklearn.model_selection import train_test_split # Split arrays or matrices into random train and test subsets\n","from sklearn.metrics import classification_report, confusion_matrix\n","import sklearn\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec # Specifies the geometry of the grid that a subplot can be placed in.\n","\n","import keras\n","from keras import models as Models\n","from keras import layers as Layers\n","from keras.preprocessing import image\n","from keras.models import Sequential,Model\n","from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization\n","from keras.layers import Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\n","from keras.optimizers import Adam\n","from keras.callbacks import ModelCheckpoint,EarlyStopping\n","from keras import utils as Utils\n","from keras.utils import to_categorical # Converts a class vector (integers) to binary class matrix.\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Dropout\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, optimizers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Split data into training and testing sets\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"Efs9_uREuGhy"},"source":["## CNN Model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":74911,"status":"ok","timestamp":1716222556807,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"MsNsmotuu-IY","outputId":"941eed01-677a-4bd1-88c8-b08a6dc5fd94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"pIGmPQdSuqZM","executionInfo":{"status":"ok","timestamp":1716222683167,"user_tz":-180,"elapsed":126377,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["import os\n","import numpy as np\n","\n","# Load data from numpy arrays\n","destination_folder = \"/content/drive/Shared drives/Computer Vision/Data/Resized_RGB_ArSL_dataset_numpy\"\n","\n","image_extensions = ['.jpg.npy', '.png.npy', '.jpeg.npy', '.JPEG.npy', '.JPG.npy', '.PNG.npy']\n","files = [file for file in os.listdir(destination_folder) if any(file.endswith(ext) for ext in image_extensions)]\n","\n","# Check if there are any image files\n","if not files:\n","    print(\"No image files found in the directory.\")\n","else:\n","    # Initialize lists to store image arrays and labels\n","    image_arrays = []\n","    labels = []\n","\n","    # Read each image file and append its numpy array and label to the lists\n","    for file in files:\n","        # Extract label from file name (assuming label is before the first underscore)\n","        label = file.split('_')[0]\n","        # Load numpy array\n","        array = np.load(os.path.join(destination_folder, file))\n","        # Append to lists\n","        image_arrays.append(array)\n","        labels.append(label)\n","\n","    # Convert lists to numpy arrays\n","    image_arrays = np.array(image_arrays)\n","    labels = np.array(labels)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"U2IFxubfEzbT","executionInfo":{"status":"ok","timestamp":1716222683170,"user_tz":-180,"elapsed":25,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["# Convert labels to numeric values\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_encoder = LabelEncoder()\n","labels_encoded = label_encoder.fit_transform(labels)\n","\n","# Split data into 70% training, 15% validation, and 15% testing\n","X_train_temp, X_test, y_train_temp, y_test = train_test_split(image_arrays, labels_encoded, test_size=0.15, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.1765, random_state=42)  # 0.1765 is approximately 15% of the remaining data"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2267,"status":"ok","timestamp":1716222685420,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"pNOBWh8-CPHw","outputId":"8db4cf54-00cd-49b6-a2c2-305fd68aabdc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 254, 254, 32)      896       \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 127, 127, 32)      0         \n"," D)                                                              \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 125, 125, 64)      18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPoolin  (None, 62, 62, 64)        0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 60, 60, 64)        36928     \n","                                                                 \n"," flatten (Flatten)           (None, 230400)            0         \n","                                                                 \n"," dense (Dense)               (None, 64)                14745664  \n","                                                                 \n"," dense_1 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 14802504 (56.47 MB)\n","Trainable params: 14802504 (56.47 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["# Preprocess data (normalize pixel values)\n","X_train = X_train / 255.0\n","X_val = X_val / 255.0\n","X_test = X_test / 255.0\n","\n","model = models.Sequential([\n","    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(image_arrays.shape[1:])),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.MaxPooling2D((2, 2)),\n","    layers.Conv2D(64, (3, 3), activation='relu'),\n","    layers.Flatten(),\n","    layers.Dense(64, activation='relu'),\n","    layers.Dense(8, activation='softmax')  # 8 classes, so softmax activation\n","])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1716222685420,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"t8m17FbWC5zd","outputId":"a8341ea1-d4eb-405a-a92b-8513ab087138"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unique labels: ['Lam' 'Meem' 'Reh' 'Seen' 'Sheen' 'Waw' 'Yeh' 'Zain']\n"]}],"source":["# Check unique labels\n","print(\"Unique labels:\", np.unique(labels))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":148960,"status":"ok","timestamp":1716222834366,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"gBFiLUsPCWs6","outputId":"978abf39-5521-43d9-a3ed-2ab609cf560f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","44/44 [==============================] - 13s 142ms/step - loss: 2.2743 - accuracy: 0.1397 - val_loss: 2.0713 - val_accuracy: 0.1894\n","Epoch 2/30\n","44/44 [==============================] - 3s 73ms/step - loss: 2.0669 - accuracy: 0.2053 - val_loss: 2.0896 - val_accuracy: 0.1860\n","Epoch 3/30\n","44/44 [==============================] - 3s 66ms/step - loss: 1.8552 - accuracy: 0.3279 - val_loss: 1.9084 - val_accuracy: 0.3588\n","Epoch 4/30\n","44/44 [==============================] - 3s 66ms/step - loss: 1.0984 - accuracy: 0.6415 - val_loss: 1.9317 - val_accuracy: 0.4086\n","Epoch 5/30\n","44/44 [==============================] - 3s 64ms/step - loss: 0.4616 - accuracy: 0.8639 - val_loss: 2.4836 - val_accuracy: 0.4252\n","Epoch 6/30\n","44/44 [==============================] - 3s 71ms/step - loss: 0.1564 - accuracy: 0.9587 - val_loss: 3.0896 - val_accuracy: 0.4419\n","Epoch 7/30\n","44/44 [==============================] - 3s 68ms/step - loss: 0.0532 - accuracy: 0.9929 - val_loss: 3.6658 - val_accuracy: 0.4319\n","Epoch 8/30\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0274 - accuracy: 0.9979 - val_loss: 4.0299 - val_accuracy: 0.4452\n","Epoch 9/30\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0174 - accuracy: 0.9986 - val_loss: 4.0708 - val_accuracy: 0.4817\n","Epoch 10/30\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0688 - accuracy: 0.9914 - val_loss: 4.0527 - val_accuracy: 0.4252\n","Epoch 11/30\n","44/44 [==============================] - 3s 69ms/step - loss: 0.0318 - accuracy: 0.9943 - val_loss: 3.7331 - val_accuracy: 0.4352\n","Epoch 12/30\n","44/44 [==============================] - 3s 70ms/step - loss: 0.0450 - accuracy: 0.9950 - val_loss: 4.3095 - val_accuracy: 0.4485\n","Epoch 13/30\n","44/44 [==============================] - 3s 65ms/step - loss: 0.0175 - accuracy: 0.9957 - val_loss: 4.9841 - val_accuracy: 0.4153\n","Epoch 14/30\n","44/44 [==============================] - 3s 68ms/step - loss: 0.0182 - accuracy: 0.9971 - val_loss: 4.9387 - val_accuracy: 0.4618\n","Epoch 15/30\n","44/44 [==============================] - 3s 69ms/step - loss: 0.0273 - accuracy: 0.9986 - val_loss: 4.2965 - val_accuracy: 0.4518\n","Epoch 16/30\n","44/44 [==============================] - 3s 75ms/step - loss: 0.0088 - accuracy: 0.9993 - val_loss: 4.5935 - val_accuracy: 0.4618\n","Epoch 17/30\n","44/44 [==============================] - 3s 73ms/step - loss: 0.0094 - accuracy: 0.9993 - val_loss: 4.5380 - val_accuracy: 0.4585\n","Epoch 18/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0085 - accuracy: 0.9986 - val_loss: 4.6171 - val_accuracy: 0.4684\n","Epoch 19/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0107 - accuracy: 0.9986 - val_loss: 4.3746 - val_accuracy: 0.4618\n","Epoch 20/30\n","44/44 [==============================] - 3s 68ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 4.6727 - val_accuracy: 0.4551\n","Epoch 21/30\n","44/44 [==============================] - 3s 70ms/step - loss: 0.0087 - accuracy: 0.9993 - val_loss: 4.2541 - val_accuracy: 0.4485\n","Epoch 22/30\n","44/44 [==============================] - 3s 69ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 4.3343 - val_accuracy: 0.4751\n","Epoch 23/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 4.3394 - val_accuracy: 0.4585\n","Epoch 24/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0050 - accuracy: 0.9993 - val_loss: 4.3856 - val_accuracy: 0.4551\n","Epoch 25/30\n","44/44 [==============================] - 3s 68ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 4.4018 - val_accuracy: 0.4551\n","Epoch 26/30\n","44/44 [==============================] - 3s 72ms/step - loss: 0.0062 - accuracy: 0.9993 - val_loss: 4.0732 - val_accuracy: 0.4618\n","Epoch 27/30\n","44/44 [==============================] - 3s 69ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 4.4062 - val_accuracy: 0.4651\n","Epoch 28/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 4.1492 - val_accuracy: 0.4618\n","Epoch 29/30\n","44/44 [==============================] - 3s 66ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 4.3226 - val_accuracy: 0.4585\n","Epoch 30/30\n","44/44 [==============================] - 3s 67ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 4.2903 - val_accuracy: 0.4651\n"]}],"source":["# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":900,"status":"ok","timestamp":1716222835247,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"E6-OEG04CYqy","outputId":"f882cee6-f6ed-44a5-afea-02c85378af45"},"outputs":[{"output_type":"stream","name":"stdout","text":["10/10 [==============================] - 0s 28ms/step - loss: 4.3877 - accuracy: 0.4385\n","Test Accuracy: 0.43853819370269775\n"]}],"source":["# Evaluate the model on the testing data\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(\"Test Accuracy:\", test_acc)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":327,"status":"ok","timestamp":1716223366380,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"8fhjBhN4DFUm","outputId":"e61059fa-cda7-4e5f-da99-31f9f14e1980"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 34ms/step\n","Predicted Label: Meem\n"]}],"source":["# Function to load and preprocess image\n","def load_and_preprocess_image(image_path, target_size=(256, 256)):\n","    try:\n","        # Load image\n","        image = cv2.imread(image_path)\n","        if image is None:\n","            raise Exception(\"Failed to load image. Please check the image file path:\", image_path)\n","        # Resize image to match target size\n","        image = cv2.resize(image, target_size)\n","        # Normalize pixel values\n","        image = image / 255.0\n","        # Return preprocessed image\n","        return image\n","    except Exception as e:\n","        print(\"Error occurred while loading the image:\", e)\n","        return None\n","\n","# Function to predict label of an image\n","def predict_image_label(image_path, model):\n","    try:\n","        # Load and preprocess image\n","        image = load_and_preprocess_image(image_path)\n","        if image is None:\n","            return None\n","        # Reshape image to match model input shape\n","        image = image.reshape(1, *image.shape)\n","        # Predict label\n","        prediction = model.predict(image)\n","        # Decode prediction to get label\n","        predicted_label = label_encoder.inverse_transform([prediction.argmax()])[0]\n","        # Return predicted label\n","        return predicted_label\n","    except Exception as e:\n","        print(\"Error occurred while predicting:\", e)\n","        return None\n","\n","def main():\n","    try:\n","        # Path to the image\n","        image_path = \"/content/5.jpg\"\n","\n","        # Predict label of the image\n","        predicted_label = predict_image_label(image_path, model)\n","\n","        # Print predicted label\n","        if predicted_label:\n","            print(\"Predicted Label:\", predicted_label)\n","    except KeyboardInterrupt:\n","        print(\"\\nExiting...\")\n","    except Exception as e:\n","        print(\"An error occurred:\", e)\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"markdown","metadata":{"id":"-ZEfcZFmXxTc"},"source":["## ResNet Model"]},{"cell_type":"markdown","metadata":{"id":"v_Wg-XNkbR6S"},"source":["## **Introduction**\n","\n","In this section, Arabic sign language gestures will be recognized from images using a ResNet model.\n","\n","### **Section Overview:**\n","* Data Preparation: The dataset comprising hand gesture images representing different signs in Arabic Sign Language is prepared. Images are loaded and preprocessed to ensure suitability for training the ResNet model.\n","* Model Building: The ResNet architecture, pretrained on ImageNet, is utilized as the base model. Additional layers are added for classification purposes. The model is trained on preprocessed images to learn patterns and features associated with different sign gestures.\n","* Model Training: The ResNet model is trained on the prepared dataset, with performance monitored over multiple epochs. Techniques such as fine-tuning and regularization are applied to improve generalization."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"-rh-lgNVakX2","executionInfo":{"status":"ok","timestamp":1716223085618,"user_tz":-180,"elapsed":339,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(image_arrays, labels_encoded, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"8b9dr-ItXbq9","executionInfo":{"status":"ok","timestamp":1716223089674,"user_tz":-180,"elapsed":3622,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"}}},"outputs":[],"source":["# Load ResNet50 model without the top layer\n","resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n","\n","# Freeze the layers in the base ResNet model\n","for layer in resnet.layers:\n","    layer.trainable = False\n","\n","# Build the model\n","model = Sequential([\n","    resnet,\n","    Flatten(),\n","    Dense(256, activation='relu'),\n","    Dropout(0.5),\n","    Dense(8, activation='softmax')  # 8 classes for Arabic Sign Language\n","])"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":161696,"status":"ok","timestamp":1716223279490,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"ydFUbbxuXgfv","outputId":"8947f450-3f70-419c-d4cc-903d6dc37473"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","50/51 [============================>.] - ETA: 0s - loss: 1.8880 - accuracy: 0.6044\n","Epoch 1: val_accuracy improved from -inf to 0.69327, saving model to best_model.h5\n","51/51 [==============================] - 15s 232ms/step - loss: 1.8852 - accuracy: 0.6047 - val_loss: 1.1130 - val_accuracy: 0.6933\n","Epoch 2/20\n","50/51 [============================>.] - ETA: 0s - loss: 1.3859 - accuracy: 0.6225\n","Epoch 2: val_accuracy did not improve from 0.69327\n","51/51 [==============================] - 9s 171ms/step - loss: 1.3850 - accuracy: 0.6228 - val_loss: 1.0140 - val_accuracy: 0.6858\n","Epoch 3/20\n","50/51 [============================>.] - ETA: 0s - loss: 1.2802 - accuracy: 0.6800\n","Epoch 3: val_accuracy improved from 0.69327 to 0.75810, saving model to best_model.h5\n","51/51 [==============================] - 19s 381ms/step - loss: 1.2794 - accuracy: 0.6789 - val_loss: 0.9787 - val_accuracy: 0.7581\n","Epoch 4/20\n","50/51 [============================>.] - ETA: 0s - loss: 1.3212 - accuracy: 0.6506\n","Epoch 4: val_accuracy did not improve from 0.75810\n","51/51 [==============================] - 9s 168ms/step - loss: 1.3190 - accuracy: 0.6509 - val_loss: 1.0528 - val_accuracy: 0.7382\n","Epoch 5/20\n","50/51 [============================>.] - ETA: 0s - loss: 1.0312 - accuracy: 0.6831\n","Epoch 5: val_accuracy improved from 0.75810 to 0.77805, saving model to best_model.h5\n","51/51 [==============================] - 10s 188ms/step - loss: 1.0297 - accuracy: 0.6833 - val_loss: 0.8028 - val_accuracy: 0.7781\n","Epoch 6/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.9856 - accuracy: 0.7150\n","Epoch 6: val_accuracy improved from 0.77805 to 0.78554, saving model to best_model.h5\n","51/51 [==============================] - 13s 262ms/step - loss: 0.9849 - accuracy: 0.7151 - val_loss: 0.7387 - val_accuracy: 0.7855\n","Epoch 7/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.9035 - accuracy: 0.7131\n","Epoch 7: val_accuracy did not improve from 0.78554\n","51/51 [==============================] - 8s 148ms/step - loss: 0.9013 - accuracy: 0.7138 - val_loss: 0.7969 - val_accuracy: 0.7606\n","Epoch 8/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.7915 - accuracy: 0.7169\n","Epoch 8: val_accuracy did not improve from 0.78554\n","51/51 [==============================] - 7s 147ms/step - loss: 0.7911 - accuracy: 0.7170 - val_loss: 0.7268 - val_accuracy: 0.7581\n","Epoch 9/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.7049 - accuracy: 0.7487\n","Epoch 9: val_accuracy did not improve from 0.78554\n","51/51 [==============================] - 9s 172ms/step - loss: 0.7054 - accuracy: 0.7488 - val_loss: 0.8848 - val_accuracy: 0.7830\n","Epoch 10/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.8615 - accuracy: 0.7475\n","Epoch 10: val_accuracy improved from 0.78554 to 0.82294, saving model to best_model.h5\n","51/51 [==============================] - 14s 282ms/step - loss: 0.8608 - accuracy: 0.7475 - val_loss: 0.6779 - val_accuracy: 0.8229\n","Epoch 11/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.8438 - accuracy: 0.7525\n","Epoch 11: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 8s 147ms/step - loss: 0.8425 - accuracy: 0.7525 - val_loss: 0.5980 - val_accuracy: 0.7880\n","Epoch 12/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.6580 - accuracy: 0.7569\n","Epoch 12: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 7s 145ms/step - loss: 0.6564 - accuracy: 0.7575 - val_loss: 0.7133 - val_accuracy: 0.8055\n","Epoch 13/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.7087 - accuracy: 0.7588\n","Epoch 13: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 9s 172ms/step - loss: 0.7084 - accuracy: 0.7587 - val_loss: 0.6260 - val_accuracy: 0.8204\n","Epoch 14/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.6599 - accuracy: 0.7625\n","Epoch 14: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 9s 169ms/step - loss: 0.6617 - accuracy: 0.7625 - val_loss: 0.6455 - val_accuracy: 0.8155\n","Epoch 15/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.6137 - accuracy: 0.7912\n","Epoch 15: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 9s 171ms/step - loss: 0.6127 - accuracy: 0.7911 - val_loss: 0.9138 - val_accuracy: 0.7731\n","Epoch 16/20\n","50/51 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.7794\n","Epoch 16: val_accuracy did not improve from 0.82294\n","51/51 [==============================] - 7s 147ms/step - loss: 0.6072 - accuracy: 0.7793 - val_loss: 0.6345 - val_accuracy: 0.8055\n","Epoch 16: early stopping\n"]}],"source":["# Compile the model\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Define callbacks\n","checkpoint = ModelCheckpoint(\"best_model.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n","\n","# Train the model\n","history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), callbacks=[checkpoint, early_stopping])"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":316,"status":"ok","timestamp":1716223347405,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"mvKqvwGRZ7tL","outputId":"32d5cd17-150c-4fec-ce2c-f564f08608a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 23ms/step\n","Predicted Label: [[6.7920346e-06 7.1729606e-01 4.5406669e-06 8.9878783e-02 4.5235618e-05\n","  3.6623948e-03 1.6555762e-01 2.3548687e-02]]\n"]}],"source":["# Load the image\n","image_path = \"/content/5.jpg\"  # Replace with the path to your image\n","\n","# Attempt to read the image\n","image = cv2.imread(image_path)\n","\n","# Check if the image was loaded successfully\n","if image is None:\n","    print(\"Error: Failed to load image.\")\n","else:\n","    # Resize the image to match the model input shape\n","    resized_image = cv2.resize(image, (256, 256))\n","\n","    # Normalize pixel values\n","    normalized_image = resized_image / 255.0\n","\n","    # Perform prediction\n","    predicted_label = model.predict(np.expand_dims(normalized_image, axis=0))\n","\n","    # Display the predicted label\n","    print(\"Predicted Label:\", predicted_label)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1716223349271,"user":{"displayName":"Shrouk Shata 201-902-199","userId":"03581611744143950125"},"user_tz":-180},"id":"AnL6_mH_cfd4","outputId":"95bee78d-2558-4ccb-affa-b8950b5399c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Label: ['Meem']\n"]}],"source":["# Decode the predicted label\n","decoded_label = label_encoder.inverse_transform(np.argmax(predicted_label, axis=1))\n","\n","# Display the predicted label\n","print(\"Predicted Label:\", decoded_label)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}