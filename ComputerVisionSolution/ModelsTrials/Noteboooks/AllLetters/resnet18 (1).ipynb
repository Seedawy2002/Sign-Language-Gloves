{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Notebook overview**"]},{"cell_type":"markdown","metadata":{},"source":["\n","This notebook is structured to implement a real-time sign language detection systemo on ArSL dataset. It includes data preprocessing, model training, and evaluation steps designed to efficiently process and recognize sign language from input data.\n"]},{"cell_type":"markdown","metadata":{"id":"J5Bfg-ibyvdF"},"source":["The following are the preprocessing techniques applied to the chosen subset of the dataset.\n","\n","1. **Image resizing**: The images are resized to (3, 224, 224) where 3 represents the number of channels (RGB) in the image and (224, 224) represents the 2D dimensions of each image.\n","\n","2. **Grayscale conversion**: Converting images to grayscale can reduce the computational complexity as it reduces the number of channels in each image from three (RGB) to one.\n","\n","\n","3. **Background Subtraction** : To focus the model on the hand gestures, it's helpful to remove or standardize the background. Techniques like thresholding or using a consistent backdrop during image capture can be effective.\n","\n","4. **Normalization**: Scaling pixel values to a range, typically between 0 and 1, helps in speeding up convergence during training.\n","\n","5. **Data Augmentation**: To make the model robust to various orientations and scales, augmenting the dataset with transformed images (e.g., rotations, scaling, translations, flipping) is beneficial."]},{"cell_type":"markdown","metadata":{"id":"hJeAYwZdCW_E"},"source":["## Needed libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:49.350251Z","iopub.status.busy":"2024-05-18T23:09:49.349646Z","iopub.status.idle":"2024-05-18T23:09:57.253649Z","shell.execute_reply":"2024-05-18T23:09:57.252854Z","shell.execute_reply.started":"2024-05-18T23:09:49.350223Z"},"id":"D30ringWCWp4","trusted":true},"outputs":[],"source":["import numpy as np\n","import cv2\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import os\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","import torchvision.transforms.v2 as transforms\n","import torchvision.models\n","from  torch.utils.data import DataLoader, Dataset\n","# from albumentations.pytorch import ToTensorV2"]},{"cell_type":"markdown","metadata":{},"source":["# **Data Preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"YmKxPu6sBFKT"},"source":["## Creating The DataFrams For All Characters"]},{"cell_type":"markdown","metadata":{},"source":["The code begins by defining the destination_folder, which is the directory containing the dataset. It then proceeds to gather and organize all image files and their associated labels into a structured format, specifically a Pandas DataFrame. This DataFrame will serve as the foundation for further data handling tasks such as preprocessing, model training, and evaluation.This code block efficiently organizes a potentially large and unstructured dataset into a manageable and easy-to-access format, setting the stage for more advanced data processing and machine learning tasks."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:57.256136Z","iopub.status.busy":"2024-05-18T23:09:57.255537Z","iopub.status.idle":"2024-05-18T23:09:59.356578Z","shell.execute_reply":"2024-05-18T23:09:59.355686Z","shell.execute_reply.started":"2024-05-18T23:09:57.256102Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>chars</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          image_path chars\n","0  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","1  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","2  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","3  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","4  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["destination_folder = \"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset\"\n","\n","\n","# Get list of .npy files in the directory\n","char_folders = [file for file in os.listdir(destination_folder)]\n","\n","# Initialize lists to store image paths and labels\n","images_paths = []\n","labels = []\n","\n","# Iterate through each character folder\n","for char_folder in char_folders:\n","    # Extract label from the folder name\n","    label = char_folder\n","    \n","    # Get the full path to the character folder\n","    full_path = os.path.join(destination_folder, char_folder)\n","    \n","    # Get all file paths within the character folder\n","    files_in_folder = [os.path.join(full_path, file) for file in os.listdir(full_path)]\n","    \n","    # Append the list of image paths to images_paths\n","    images_paths.extend(files_in_folder)\n","    \n","    # Append the label to the labels list\n","    labels.extend([label] * len(files_in_folder))\n","\n","    \n","data=pd.DataFrame({\"image_path\":images_paths,\"chars\":labels})\n","data.head()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Mapping Char to Numbers"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.358164Z","iopub.status.busy":"2024-05-18T23:09:59.357832Z","iopub.status.idle":"2024-05-18T23:09:59.377395Z","shell.execute_reply":"2024-05-18T23:09:59.376247Z","shell.execute_reply.started":"2024-05-18T23:09:59.358139Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>chars</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          image_path chars  label\n","0  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain      0\n","1  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain      0\n","2  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain      0\n","3  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain      0\n","4  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain      0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["data['label'] = data['chars'].apply(lambda x: char_folders.index(x))\n","\n","data.head()"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T02:20:51.655874Z","iopub.status.busy":"2024-05-19T02:20:51.655080Z","iopub.status.idle":"2024-05-19T02:20:51.663226Z","shell.execute_reply":"2024-05-19T02:20:51.662289Z","shell.execute_reply.started":"2024-05-19T02:20:51.655844Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'Zain',\n"," 1: 'Zah',\n"," 2: 'Meem',\n"," 3: 'Seen',\n"," 4: 'Teh',\n"," 5: 'Lam',\n"," 6: 'Dad',\n"," 7: 'Teh_Marbuta',\n"," 8: 'Reh',\n"," 9: 'Sad',\n"," 10: 'Dal',\n"," 11: 'Sheen',\n"," 12: 'Hah',\n"," 13: 'Beh',\n"," 14: 'Tah',\n"," 15: 'Alef',\n"," 16: 'Waw',\n"," 17: 'Qaf',\n"," 18: 'Al',\n"," 19: 'Ghain',\n"," 20: 'Heh',\n"," 21: 'Ain',\n"," 22: 'Kaf',\n"," 23: 'Thal',\n"," 24: 'Feh',\n"," 25: 'Khah',\n"," 26: 'Yeh',\n"," 27: 'Jeem',\n"," 28: 'Theh',\n"," 29: 'Noon',\n"," 30: 'Laa'}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["mapping={}\n","for i in range(len(char_folders)):\n","    mapping[i]=char_folders[i]\n","mapping"]},{"cell_type":"markdown","metadata":{},"source":["## Data Preparation and Encoding"]},{"cell_type":"markdown","metadata":{},"source":["This section of the notebook focuses on preparing the dataset for training. It includes splitting the data into training and testing sets."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.380610Z","iopub.status.busy":"2024-05-18T23:09:59.380236Z","iopub.status.idle":"2024-05-18T23:09:59.402700Z","shell.execute_reply":"2024-05-18T23:09:59.401876Z","shell.execute_reply.started":"2024-05-18T23:09:59.380580Z"},"id":"ynAm_QT6TK-e","trusted":true},"outputs":[],"source":["train_data, test_data= train_test_split(data, test_size=0.2, stratify=labels)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.404410Z","iopub.status.busy":"2024-05-18T23:09:59.403996Z","iopub.status.idle":"2024-05-18T23:09:59.410509Z","shell.execute_reply":"2024-05-18T23:09:59.409366Z","shell.execute_reply.started":"2024-05-18T23:09:59.404380Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape  (6284, 3) test data shape (1572, 3)\n"]}],"source":["print(\"train data shape \",train_data.shape,\"test data shape\" ,test_data.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Custom Dataset Handling and Image Preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["This section focuses on creating a custom dataset class and applying a series of image transformations for preprocessing. The primary goal here is to ensure that the images are properly formatted and augmented to enhance the model's ability to generalize from the training data."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.412466Z","iopub.status.busy":"2024-05-18T23:09:59.411988Z","iopub.status.idle":"2024-05-18T23:09:59.421347Z","shell.execute_reply":"2024-05-18T23:09:59.420370Z","shell.execute_reply.started":"2024-05-18T23:09:59.412438Z"},"id":"TPiBe5HHTviG","trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data, transforms=None):\n","        self.data = data\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row=self.data.iloc[idx]\n","        image_path = row['image_path']\n","        label = row['label']\n","        char=row['chars']\n","        # Read the image using OpenCV\n","        image = cv2.imread(image_path)\n","        # Convert the image from BGR to RGB (OpenCV uses BGR by default)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # Convert the image to a PIL Image\n","        image = Image.fromarray(image)\n","        \n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        return image, label,char"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-05-18T23:09:59.422993Z","iopub.status.busy":"2024-05-18T23:09:59.422646Z","iopub.status.idle":"2024-05-18T23:09:59.434167Z","shell.execute_reply":"2024-05-18T23:09:59.433172Z","shell.execute_reply.started":"2024-05-18T23:09:59.422965Z"},"id":"8Sk0_kxrat5T","outputId":"da9698f8-e0d5-4c01-fe27-17a2d3f80ab8","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n","  warnings.warn(\n"]}],"source":["# Can add multiple transformations to enhance the training of the model\n","\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(30),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n","])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.435739Z","iopub.status.busy":"2024-05-18T23:09:59.435386Z","iopub.status.idle":"2024-05-18T23:09:59.442232Z","shell.execute_reply":"2024-05-18T23:09:59.441047Z","shell.execute_reply.started":"2024-05-18T23:09:59.435704Z"},"id":"4XqGe9zVUK4h","trusted":true},"outputs":[],"source":["trainset = MyDataset(train_data,train_transforms)\n","trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","testset = MyDataset(test_data,test_transforms)\n","testloader = DataLoader(testset, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## **Comprehensive Model Setup, Training, and Evaluation**"]},{"cell_type":"markdown","metadata":{},"source":["This section outlines the setup, training, and evaluation of ResNet-18 which is a deep learning model from the Residual Network family, distinguished by its 18-layer architecture. It incorporates residual connections that prevent the vanishing gradient problem, facilitating effective learning even in deep networks. Commonly used for image classification, . Initially, the model is loaded with pre-trained weights and adjusted to match the specific requirements of the dataset by modifying its final layers. The training process is facilitated by defining an appropriate loss function (cross-entropy) and an optimizer (Adam), which are crucial for optimizing the model's performance. The section includes detailed training and validation functions that operate over multiple epochs to incrementally improve accuracy and reduce loss. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 123MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=31, bias=True)\n",")\n"]}],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model_resnet18 = torchvision.models.resnet18(weights=\"DEFAULT\")\n","for name, param in model_resnet18.named_parameters():\n","    if \"layer4\" in name or \"fc\" in name:\n","        param.requires_grad = True\n","    else:\n","        param.requires_grad = False\n","\n","model_resnet18.fc = nn.Linear(512, 31)\n","model_resnet18.to(device)\n","print(model_resnet18)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.444407Z","iopub.status.busy":"2024-05-18T23:09:59.443964Z","iopub.status.idle":"2024-05-18T23:09:59.453397Z","shell.execute_reply":"2024-05-18T23:09:59.452433Z","shell.execute_reply.started":"2024-05-18T23:09:59.444369Z"},"id":"iI-s0bvE0zg0","trusted":true},"outputs":[],"source":["def train(model, dataloader, loss, optimizer,device='cuda'):\n","    model.train()\n","    acc = []\n","    lss_history = []\n","    for data,labels,chars in dataloader:\n","        optimizer.zero_grad()\n","        data=data.to(device)\n","        labels=labels.to(device)\n","        pred = model(data)\n","        lss = loss(pred, labels)\n","        lss.backward()\n","        optimizer.step()\n","        # acc calculations\n","        lss_history.append(lss.item())\n","        acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\n","    return np.mean(lss_history) ,np.mean(acc)\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:09:59.457127Z","iopub.status.busy":"2024-05-18T23:09:59.456259Z","iopub.status.idle":"2024-05-18T23:09:59.464588Z","shell.execute_reply":"2024-05-18T23:09:59.463674Z","shell.execute_reply.started":"2024-05-18T23:09:59.457094Z"},"trusted":true},"outputs":[],"source":["# function to validate the model\n","def validate(model, dataloader, loss_func,device='cuda'):\n","    model.eval()\n","    loss_values = []\n","    acc_values = []\n","    with torch.no_grad():\n","        for data,labels,chars in dataloader:\n","            data=data.to(device)\n","            labels=labels.to(device)\n","            pred = model(data)\n","            loss = loss_func(pred, labels)\n","            loss_values.append(loss.item())\n","            acc_value = (pred.argmax(axis = 1) == labels).type(torch.float32)\n","            acc_values.append(acc_value.mean().item())\n","    return np.mean(loss_values), np.mean(acc_values)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:10:00.493824Z","iopub.status.busy":"2024-05-18T23:10:00.493522Z","iopub.status.idle":"2024-05-18T23:10:00.499299Z","shell.execute_reply":"2024-05-18T23:10:00.498350Z","shell.execute_reply.started":"2024-05-18T23:10:00.493797Z"},"id":"y5yxKRcp3fFb","trusted":true},"outputs":[],"source":["loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model_resnet18.parameters(), lr=0.0001)"]},{"cell_type":"markdown","metadata":{},"source":["# **Results Overview from Initial Training Cycles**"]},{"cell_type":"markdown","metadata":{},"source":["The results for the ResNet-18 model over 10 training epochs demonstrate a consistent improvement, with training loss decreasing from 2.182 to 0.896 and accuracy increasing to 59.8%. This steady progression reflects the model's effective learning and generalization to the training data. Upon testing, the model showcased excellent performance with a test loss of 0.216 and an impressive accuracy of 93.4%, indicating robust generalization to unseen data. These outcomes highlight ResNet-18's capability in handling complex image classification tasks, suggesting it has been well-adapted and tuned for the dataset while maintaining a balance against overfitting."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:10:00.501003Z","iopub.status.busy":"2024-05-18T23:10:00.500639Z","iopub.status.idle":"2024-05-18T23:10:00.508240Z","shell.execute_reply":"2024-05-18T23:10:00.507258Z","shell.execute_reply.started":"2024-05-18T23:10:00.500970Z"},"id":"OfIt-oMo1Vg5","trusted":true},"outputs":[],"source":["def tune_model(epochs, model, train_dataloader, test_dataloader, loss_func, optimizer):\n","    for epoch in range(epochs):\n","        train_loss, train_acc = train(model, train_dataloader, loss_func, optimizer)\n","        print(f\"Epoch : {epoch + 1} || Train loss : {train_loss:5.3f} || Train accuracy : {train_acc:5.3f}\", end=\"\")\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-05-18T23:10:00.509625Z","iopub.status.busy":"2024-05-18T23:10:00.509364Z","iopub.status.idle":"2024-05-19T01:57:59.434583Z","shell.execute_reply":"2024-05-19T01:57:59.433676Z","shell.execute_reply.started":"2024-05-18T23:10:00.509593Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1 || Train loss : 2.102 || Train accuracy : 0.471"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 2 || Train loss : 0.812 || Train accuracy : 0.821"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 3 || Train loss : 0.485 || Train accuracy : 0.893"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 4 || Train loss : 0.348 || Train accuracy : 0.925"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 5 || Train loss : 0.251 || Train accuracy : 0.945"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 6 || Train loss : 0.205 || Train accuracy : 0.955"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 7 || Train loss : 0.159 || Train accuracy : 0.964"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 8 || Train loss : 0.129 || Train accuracy : 0.973"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 9 || Train loss : 0.113 || Train accuracy : 0.979"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 10 || Train loss : 0.096 || Train accuracy : 0.980"]}],"source":["tune_model(10, model_resnet18, trainloader, testloader, loss, optimizer)\n","torch.save(model_resnet18, f\"/kaggle/working/resnet18 with epoch{epoch+1}.pth\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## **Testing The Model**"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T02:15:17.975550Z","iopub.status.busy":"2024-05-19T02:15:17.975300Z","iopub.status.idle":"2024-05-19T02:19:34.087811Z","shell.execute_reply":"2024-05-19T02:19:34.086912Z","shell.execute_reply.started":"2024-05-19T02:15:17.975528Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":[" Train loss : 0.216 || Train accuracy : 0.934\n"]}],"source":["test_loss, test_acc= validate(model_resnet18, testloader, loss)\n","print(f\" Train loss : {test_loss:5.3f} || Train accuracy : {test_acc:5.3f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":2852448,"sourceId":6116155,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
