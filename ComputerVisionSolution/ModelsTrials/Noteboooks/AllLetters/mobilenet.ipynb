{"cells":[{"cell_type":"markdown","metadata":{"id":"QbF_yvAl-9rL"},"source":["# **Notebook overview**"]},{"cell_type":"markdown","metadata":{"id":"oY88qkRkx7la"},"source":["\n","This notebook is structured to implement a real-time sign language detection systemo on ArSL dataset. It includes data preprocessing, model training, and evaluation steps designed to efficiently process and recognize sign language from input data.\n"]},{"cell_type":"markdown","metadata":{"id":"J5Bfg-ibyvdF"},"source":["The following are the preprocessing techniques applied to the chosen subset of the dataset.\n","\n","1. **Image resizing**: The images are resized to (3, 224, 224) where 3 represents the number of channels (RGB) in the image and (224, 224) represents the 2D dimensions of each image.\n","\n","2. **Grayscale conversion**: Converting images to grayscale can reduce the computational complexity as it reduces the number of channels in each image from three (RGB) to one.\n","\n","\n","3. **Background Subtraction** : To focus the model on the hand gestures, it's helpful to remove or standardize the background. Techniques like thresholding or using a consistent backdrop during image capture can be effective.\n","\n","4. **Normalization**: Scaling pixel values to a range, typically between 0 and 1, helps in speeding up convergence during training.\n","\n","5. **Data Augmentation**: To make the model robust to various orientations and scales, augmenting the dataset with transformed images (e.g., rotations, scaling, translations, flipping) is beneficial."]},{"cell_type":"markdown","metadata":{"id":"hJeAYwZdCW_E"},"source":["## Needed libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:05.739977Z","iopub.status.busy":"2024-05-19T22:57:05.739690Z","iopub.status.idle":"2024-05-19T22:57:05.745894Z","shell.execute_reply":"2024-05-19T22:57:05.745019Z","shell.execute_reply.started":"2024-05-19T22:57:05.739951Z"},"id":"D30ringWCWp4","trusted":true},"outputs":[],"source":["import numpy as np\n","import cv2\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","import os\n","import torch\n","import pandas as pd\n","import torch.nn as nn\n","import torchvision.transforms.v2 as transforms\n","import torchvision.models\n","from  torch.utils.data import DataLoader, Dataset\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"markdown","metadata":{"id":"ay3LQPHGh2bI"},"source":["# **Data Preprocessing**"]},{"cell_type":"markdown","metadata":{"id":"YmKxPu6sBFKT"},"source":["## Creating The DataFrams For All Characters"]},{"cell_type":"markdown","metadata":{"id":"A7X9OWN9h2bJ"},"source":["The code begins by defining the destination_folder, which is the directory containing the dataset. It then proceeds to gather and organize all image files and their associated labels into a structured format, specifically a Pandas DataFrame. This DataFrame will serve as the foundation for further data handling tasks such as preprocessing, model training, and evaluation.This code block efficiently organizes a potentially large and unstructured dataset into a manageable and easy-to-access format, setting the stage for more advanced data processing and machine learning tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:05.765427Z","iopub.status.busy":"2024-05-19T22:57:05.764516Z","iopub.status.idle":"2024-05-19T22:57:06.876512Z","shell.execute_reply":"2024-05-19T22:57:06.875559Z","shell.execute_reply.started":"2024-05-19T22:57:05.765394Z"},"trusted":true,"id":"SKCm7Equh2bJ","outputId":"5b6c6c9f-d6f1-478a-e6c5-20a824bec413"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_path</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>/kaggle/input/rgb-arabic-alphabets-sign-langua...</td>\n","      <td>Zain</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          image_path label\n","0  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","1  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","2  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","3  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain\n","4  /kaggle/input/rgb-arabic-alphabets-sign-langua...  Zain"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["destination_folder = \"/kaggle/input/rgb-arabic-alphabets-sign-language-dataset/RGB ArSL dataset\"\n","\n","\n","# Get list of .npy files in the directory\n","char_folders = [file for file in os.listdir(destination_folder)]\n","\n","# Initialize lists to store image paths and labels\n","images_paths = []\n","labels = []\n","\n","# Iterate through each character folder\n","for char_folder in char_folders:\n","    # Extract label from the folder name\n","    label = char_folder\n","\n","    # Get the full path to the character folder\n","    full_path = os.path.join(destination_folder, char_folder)\n","\n","    # Get all file paths within the character folder\n","    files_in_folder = [os.path.join(full_path, file) for file in os.listdir(full_path)]\n","\n","    # Append the list of image paths to images_paths\n","    images_paths.extend(files_in_folder)\n","\n","    # Append the label to the labels list\n","    labels.extend([label] * len(files_in_folder))\n","\n","\n","data=pd.DataFrame({\"image_path\":images_paths,\"label\":labels})\n","data.head()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e0-XOCv7vEKj"},"source":["## Data Preparation and Encoding"]},{"cell_type":"markdown","metadata":{"id":"tBNRMmMuh2bK"},"source":["This section of the notebook focuses on preparing the dataset for training. It includes splitting the data into training and testing sets, encoding the labels, and resetting the indices of the DataFrames to ensure clean and organized data. Each step is crucial for setting up a structured and efficient data pipeline, which is essential for the successful training and evaluation of machine learning models.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.878506Z","iopub.status.busy":"2024-05-19T22:57:06.877931Z","iopub.status.idle":"2024-05-19T22:57:06.899518Z","shell.execute_reply":"2024-05-19T22:57:06.898608Z","shell.execute_reply.started":"2024-05-19T22:57:06.878478Z"},"id":"ynAm_QT6TK-e","trusted":true},"outputs":[],"source":["train_data,test_data,train_labels,test_labels= train_test_split(data['image_path'],data['label'], test_size=0.2, stratify=labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HbuxCNlZh2bL"},"outputs":[],"source":["# One hot encoding\n","\n","label_encoder = LabelEncoder()\n","train_labels = label_encoder.fit_transform(train_labels)\n","test_labels = label_encoder.fit_transform(test_labels)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"re22CMMbh2bM"},"outputs":[],"source":["# Resetting indices\n","train_data = train_data.reset_index(drop=True)\n","test_data = test_data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.901145Z","iopub.status.busy":"2024-05-19T22:57:06.900788Z","iopub.status.idle":"2024-05-19T22:57:06.906826Z","shell.execute_reply":"2024-05-19T22:57:06.905710Z","shell.execute_reply.started":"2024-05-19T22:57:06.901119Z"},"trusted":true,"id":"YrYE-baFh2bM","outputId":"212af7ec-cc41-4802-84e8-dc3adb86f58f"},"outputs":[{"name":"stdout","output_type":"stream","text":["train data shape  (6284,) test data shape (1572,)\n"]}],"source":["print(\"train data shape \",train_data.shape,\"test data shape\" ,test_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"zkIUBqZIgLjP"},"source":["### Custom Dataset Handling and Image Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"ulBIORCuh2bN"},"source":["This section focuses on creating a custom dataset class and applying a series of image transformations for preprocessing. The primary goal here is to ensure that the images are properly formatted and augmented to enhance the model's ability to generalize from the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.910250Z","iopub.status.busy":"2024-05-19T22:57:06.909843Z","iopub.status.idle":"2024-05-19T22:57:06.918403Z","shell.execute_reply":"2024-05-19T22:57:06.917443Z","shell.execute_reply.started":"2024-05-19T22:57:06.910217Z"},"id":"TPiBe5HHTviG","trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, image,labels, transforms=None):\n","        self.image = image\n","        self.labels=labels\n","        self.transforms = transforms\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        label = self.labels[idx]\n","        # Read the image using OpenCV\n","        image = cv2.imread(self.image[idx])\n","        # Convert the image from BGR to RGB (OpenCV uses BGR by default)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # Convert the image to a PIL Image\n","        image = Image.fromarray(image)\n","\n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.940544Z","iopub.status.busy":"2024-05-19T22:57:06.940113Z","iopub.status.idle":"2024-05-19T22:57:06.949739Z","shell.execute_reply":"2024-05-19T22:57:06.948564Z","shell.execute_reply.started":"2024-05-19T22:57:06.940456Z"},"id":"8Sk0_kxrat5T","outputId":"da9698f8-e0d5-4c01-fe27-17a2d3f80ab8","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:43: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n","  warnings.warn(\n"]}],"source":["# Can add multiple transformations to enhance the training of the model\n","train_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(30),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.951289Z","iopub.status.busy":"2024-05-19T22:57:06.950957Z","iopub.status.idle":"2024-05-19T22:57:06.958613Z","shell.execute_reply":"2024-05-19T22:57:06.957625Z","shell.execute_reply.started":"2024-05-19T22:57:06.951262Z"},"id":"4XqGe9zVUK4h","trusted":true},"outputs":[],"source":["trainset = MyDataset(train_data,train_labels,train_transforms)\n","trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","testset = MyDataset(test_data,test_labels,test_transforms)\n","testloader = DataLoader(testset, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"76hpoDMgh2bP"},"source":["## **Comprehensive Model Setup, Training, and Evaluation**"]},{"cell_type":"markdown","metadata":{"id":"2KE5YTx8h2bQ"},"source":["This section outlines the setup, training, and evaluation of a MobileNet V2 model whish is a lightweight deep neural network known for its efficiency on mobile devices with limited computational power, adapted for image classification tasks. Initially, the model is loaded with pre-trained weights and adjusted to match the specific requirements of the dataset by modifying its final layers. The training process is facilitated by defining an appropriate loss function (cross-entropy) and an optimizer (Adam), which are crucial for optimizing the model's performance. The section includes detailed training and validation functions that operate over multiple epochs to incrementally improve accuracy and reduce loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PEdXKS8h2bQ","outputId":"85c07060-62e4-48a5-d6a3-fdcb5aba8770"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 45.6MB/s]\n"]}],"source":["import torchvision.models as models\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","model_mobilenet = models.mobilenet_v2(weights=True)\n","for param in model_mobilenet.parameters():\n","    param.requires_grad = False\n","\n","# Replace the last fully connected layer for our specific case\n","model_mobilenet.classifier[1] = nn.Linear(model_mobilenet.last_channel, 31)\n","model_mobilenet = model_mobilenet.to(device)\n","\n","# Loss function and optimizer\n","loss = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model_mobilenet.parameters(), lr=0.001)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.960456Z","iopub.status.busy":"2024-05-19T22:57:06.960179Z","iopub.status.idle":"2024-05-19T22:57:06.967826Z","shell.execute_reply":"2024-05-19T22:57:06.966823Z","shell.execute_reply.started":"2024-05-19T22:57:06.960431Z"},"id":"iI-s0bvE0zg0","trusted":true},"outputs":[],"source":["def train(model, dataloader, loss, optimizer,device='cuda'):\n","    model.train()\n","    acc = []\n","    lss_history = []\n","    for data,labels in dataloader:\n","        data=data.to(device)\n","        labels=labels.to(device)\n","        optimizer.zero_grad()\n","\n","        pred = model(data)\n","        lss = loss(pred, labels)\n","        lss.backward()\n","        optimizer.step()\n","        # acc calculations\n","        lss_history.append(lss.item())\n","        acc.append(((pred.argmax(axis = 1) == labels).type(torch.float)).mean().item())\n","    return np.mean(lss_history) ,np.mean(acc)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:06.971726Z","iopub.status.busy":"2024-05-19T22:57:06.971422Z","iopub.status.idle":"2024-05-19T22:57:06.980390Z","shell.execute_reply":"2024-05-19T22:57:06.979466Z","shell.execute_reply.started":"2024-05-19T22:57:06.971694Z"},"trusted":true,"id":"ljGcZFI-h2bQ"},"outputs":[],"source":["# function to validate the model\n","def validate(model, dataloader, loss_func,device='cuda'):\n","    model.eval()\n","    loss_values = []\n","    acc_values = []\n","    with torch.no_grad():\n","        for data,labels in dataloader:\n","            data=data.to(device)\n","            labels=labels.to(device)\n","            pred = model(data)\n","            loss = loss_func(pred, labels)\n","            loss_values.append(loss.item())\n","            acc_value = (pred.argmax(axis = 1) == labels).type(torch.float32)\n","            acc_values.append(acc_value.mean().item())\n","    return np.mean(loss_values), np.mean(acc_values)"]},{"cell_type":"markdown","metadata":{"id":"aw0mq2nSh2bR"},"source":["# **Results Overview from Initial Training Cycles**"]},{"cell_type":"markdown","metadata":{"id":"bE8Qoczxh2bR"},"source":["The results from the initial epochs show that the MobileNet V2 model is learning effectively, evidenced by decreasing training loss and modestly improving accuracy. Over six epochs, training loss dropped from 2.949 to 1.573, and training accuracy increased from 19.7% to 54.2%. Test accuracy peaked at 49.9% in the fourth epoch but showed signs of potential overfitting with a slight decrease afterwards.\n","\n","These outcomes highlight that while the model is on the right track, it's still in the early stages of training. Extending the number of epochs and monitoring for overfitting are advisable to stabilize and enhance the model's performance.\n","\n","We were unable to extend training due to the slow performance of the model under current conditions. This limitation underscores the need for more efficient computational resources or model optimization techniques to facilitate longer training periods and improve model outcomes."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:07.619339Z","iopub.status.busy":"2024-05-19T22:57:07.618442Z","iopub.status.idle":"2024-05-19T22:57:07.625422Z","shell.execute_reply":"2024-05-19T22:57:07.624279Z","shell.execute_reply.started":"2024-05-19T22:57:07.619303Z"},"id":"OfIt-oMo1Vg5","trusted":true},"outputs":[],"source":["def tune_model(epochs, model, train_dataloader, test_dataloader, loss_func, optimizer):\n","    for epoch in range(epochs):\n","        train_loss, train_acc = train(model, train_dataloader, loss_func, optimizer,device=device)\n","        test_loss, test_acc= validate(model, test_dataloader, loss,device=device)\n","        print(f\"Epoch : {epoch + 1} || Train loss : {train_loss:5.3f} || Train accuracy : {train_acc:5.3f}\", end=\"\")\n","        print(f\" Test loss : {test_loss:5.3f} || Test accuracy : {test_acc:5.3f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-19T22:57:07.627717Z","iopub.status.busy":"2024-05-19T22:57:07.627065Z"},"trusted":true,"id":"zqqQiVZ4h2bS","outputId":"a30d2d18-f01c-4e9d-bd50-78b7153e9417"},"outputs":[{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 1 || Train loss : 2.949 || Train accuracy : 0.197 Test loss : 2.557 || Test accuracy : 0.294\n"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 2 || Train loss : 2.234 || Train accuracy : 0.387 Test loss : 2.147 || Test accuracy : 0.410\n"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 3 || Train loss : 1.913 || Train accuracy : 0.463 Test loss : 1.933 || Test accuracy : 0.461\n"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 4 || Train loss : 1.764 || Train accuracy : 0.492 Test loss : 1.786 || Test accuracy : 0.499\n"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 5 || Train loss : 1.665 || Train accuracy : 0.511 Test loss : 1.773 || Test accuracy : 0.490\n"]},{"name":"stderr","output_type":"stream","text":["Premature end of JPEG file\n"]},{"name":"stdout","output_type":"stream","text":["Epoch : 6 || Train loss : 1.573 || Train accuracy : 0.542 Test loss : 1.739 || Test accuracy : 0.486\n"]}],"source":["tune_model(10, model_mobilenet, trainloader, testloader, loss, optimizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"RKQrRGKyh2bS"},"outputs":[],"source":["# Load the saved model weights\n","model_mobilenet.cpu()\n","torch.save(model_mobilenet, f\"/kaggle/working/model_mobilenet_cpu.pth\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2852448,"sourceId":6116155,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}